{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495ef5e3",
   "metadata": {},
   "source": [
    "### <font color='red'>Welcome to the fNIRS Pipeline!</font>\n",
    "\n",
    "Welcome to the pipeline for fNIRS data analysis! Here we are working on the pre-processing and post-processing of fNIRS. This following pipeline works on several tasks that are needed throughout the process. There are instructions and tips given below that can help you run the pipeline to use it for your fnirs data. Make sure to always follow the output of each code in the end to know how to proceed with the pipeline\n",
    "\n",
    "---------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "### <font color=\"tomato\">Instructions</font> \n",
    "\n",
    "Here we are working on <font color=\"purple\">**jupyter notebook**</font>, so the use of code blocks and kernels are important:\n",
    "\n",
    "###### <font color='dodgerblue'>**Code Blocks**</font>\n",
    "Code blocks are blocks which contains the code to execute the code inside it. \n",
    "\n",
    "###### <font color='dodgerblue'>**Kernels**</font>\n",
    "Kernels are the environment on which the code runs. Two tips: \n",
    "\n",
    "1. Sometimes it takes time for a code to be executed, that is when you can stop the kernel by clicking <font color='teal'>**the black square**</font> button on the right of the run button. Then you can run the block again. \n",
    "\n",
    "2. Sometimes maybe there will be a need to re run the whole pipeline again, that is when you can go to Kernels and select <font color='teal'>**Restart & Clear Outputs**</font> to restart the kernel.\n",
    "\n",
    "\n",
    "###### <font color='dodgerblue'>**Shortcuts on keyboards**</font>\n",
    "To run a block - <kbd>Ctrl + Enter</kbd></br>\n",
    "To run a block and go automatically to the next block - <kbd>Shift + Enter</kbd> \n",
    "\n",
    "###### <font color='dodgerblue'>**Visualizations**</font>\n",
    "Visualizations for each stages will come in a pop-up or a different window. You can use your keyboard and press <kbd>&larr; and  &rarr;</kbd> to go left and right to see the plot horizontally and also you can use <kbd>&uarr; and &darr;</kbd> to go up and down to see the plot vertically, inside the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb22941",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------\n",
    "### <font color='mediumseagreen'>Start from below</font>\n",
    "You can start the pipeline by running the code in the next block. It will import all packages neeeded for the pipeline.\n",
    "\n",
    "Please run the next two code blocks first to import the packages needed for this pipeline (Press <kbd>Ctrl + Enter</kbd> to run or <kbd>Shift + Enter</kbd> to run and move to the next block)\n",
    "\n",
    "Once you start running the code blocks, make sure you **<font color=\"tomato\">see the outputs</font>** and **<font color=\"tomato\">follow</font>** what it says as it will tell you what to do next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba590ab1",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f90e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in c:\\users\\yameen\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne) (22.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne) (1.23.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne) (4.64.1)\n",
      "Requirement already satisfied: pooch>=1.5 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne) (1.7.0)\n",
      "Requirement already satisfied: matplotlib>=3.5.0 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne) (3.7.0)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne) (0.3)\n",
      "Requirement already satisfied: scipy>=1.7.1 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne) (1.10.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne) (3.1.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->mne) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->mne) (1.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->mne) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->mne) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->mne) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->mne) (9.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->mne) (4.25.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from pooch>=1.5->mne) (2.28.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from pooch>=1.5->mne) (2.5.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from jinja2->mne) (2.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from tqdm->mne) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->mne) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\yameen\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\yameen\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\yameen\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\yameen\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\yameen\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\yameen\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne_nirs in c:\\users\\yameen\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: scipy>=0.17.1 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne_nirs) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne_nirs) (1.23.5)\n",
      "Requirement already satisfied: nilearn>=0.9 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne_nirs) (0.10.0)\n",
      "Requirement already satisfied: mne>=1.0 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne_nirs) (1.6.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne_nirs) (0.12.2)\n",
      "Requirement already satisfied: h5io>=0.1.7 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne_nirs) (0.1.7)\n",
      "Requirement already satisfied: h5py in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from h5io>=0.1.7->mne_nirs) (3.7.0)\n",
      "Requirement already satisfied: pooch>=1.5 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne>=1.0->mne_nirs) (1.7.0)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne>=1.0->mne_nirs) (0.3)\n",
      "Requirement already satisfied: decorator in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne>=1.0->mne_nirs) (5.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne>=1.0->mne_nirs) (22.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne>=1.0->mne_nirs) (3.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne>=1.0->mne_nirs) (4.64.1)\n",
      "Requirement already satisfied: matplotlib>=3.5.0 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from mne>=1.0->mne_nirs) (3.7.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from nilearn>=0.9->mne_nirs) (4.9.1)\n",
      "Requirement already satisfied: nibabel>=3.2.0 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from nilearn>=0.9->mne_nirs) (5.1.0)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from nilearn>=0.9->mne_nirs) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from nilearn>=0.9->mne_nirs) (1.2.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from nilearn>=0.9->mne_nirs) (1.1.1)\n",
      "Requirement already satisfied: requests>=2.25.0 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from nilearn>=0.9->mne_nirs) (2.28.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->mne>=1.0->mne_nirs) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->mne>=1.0->mne_nirs) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->mne>=1.0->mne_nirs) (4.25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->mne>=1.0->mne_nirs) (9.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->mne>=1.0->mne_nirs) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->mne>=1.0->mne_nirs) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from matplotlib>=3.5.0->mne>=1.0->mne_nirs) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->nilearn>=0.9->mne_nirs) (2022.7)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from pooch>=1.5->mne>=1.0->mne_nirs) (2.5.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from requests>=2.25.0->nilearn>=0.9->mne_nirs) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from requests>=2.25.0->nilearn>=0.9->mne_nirs) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from requests>=2.25.0->nilearn>=0.9->mne_nirs) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from requests>=2.25.0->nilearn>=0.9->mne_nirs) (2022.12.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->nilearn>=0.9->mne_nirs) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from jinja2->mne>=1.0->mne_nirs) (2.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from tqdm->mne>=1.0->mne_nirs) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yameen\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->mne>=1.0->mne_nirs) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\yameen\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\yameen\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\yameen\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\yameen\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\yameen\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\yameen\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install mne\n",
    "!pip install mne_nirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e9a8a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yameen\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! MNE and visualization packages has been imported into the pipeline!\n",
      "Next step: Make sure you have your dataset in this directory in your local desktop as it shows - C:/Users/YourUserName/mne_data/Yourfolder/Yourfile\n",
      "You can now move to importing the dataset into the pipeline! Run the next block.\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import mne_nirs\n",
    "\n",
    "#For ignoring depreciations\n",
    "import warnings\n",
    "from cryptography.utils import CryptographyDeprecationWarning\n",
    "warnings.filterwarnings(action='ignore', category=CryptographyDeprecationWarning)\n",
    "\n",
    "#Import required libraries - os for interacting with the os and matplotlib for visualizations\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib notebook\n",
    "%matplotlib qt\n",
    "\n",
    "print(\"Great! MNE and visualization packages has been imported into the pipeline!\")\n",
    "print(\"Next step: Make sure you have your dataset in this directory in your local desktop as it shows - C:/Users/YourUserName/mne_data/Yourfolder/Yourfile\")\n",
    "print(\"You can now move to importing the dataset into the pipeline! Run the next block.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b72d4",
   "metadata": {},
   "source": [
    "---------------------------------------------\n",
    "#### Importing the file\n",
    "\n",
    "Take a look at the filepath here below and make sure it matches your file path. Make changes in the code (if needed), to match your path - <strong><font color=\"dodgerblue\">paste your path there inside the inverted commas in the filepath variable.</font></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11629f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the current filepath: C:/Users/Yameen/mne_data/infinity-walking-data/2021-10-16_001/\n",
      "\n",
      "Once you are done with the filepath, you can move to the next block, make sure to run this block one more time if you have made changes to the file path\n"
     ]
    }
   ],
   "source": [
    "filepath=\"C:/Users/Yameen/mne_data/infinity-walking-data/2021-10-16_001/\"\n",
    "\n",
    "print(\"This is the current filepath: \" + filepath + \"\\n\")\n",
    "print(\"Once you are done with the filepath, you can move to the next block, make sure to run this block one more time if you have made changes to the file path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "406a09d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the name of the snirf file which contains the dataset you want to work on i.e. the file should end with .snirf: \n",
      "2021-10-16_001.snirf\n",
      "\n",
      "Take a look at the directory here and make sure it matches your file path\n",
      "\n",
      "C:/Users/Yameen/mne_data/infinity-walking-data/2021-10-16_001/2021-10-16_001.snirf\n",
      "\n",
      "Awesome! The file has been imported into the pipeline! Make sure you run the predefined functions block and then move to the next block to load your data!\n"
     ]
    }
   ],
   "source": [
    "filename = input(\"Please enter the name of the snirf file which contains the dataset you want to work on i.e. the file should end with .snirf: \" + \"\\n\")\n",
    "\n",
    "#Defining the location of the dataset\n",
    "print(\"\\nTake a look at the directory here and make sure it matches your file path\\n\")\n",
    "fname = filepath + filename \n",
    "print(fname)\n",
    "\n",
    "print(\"\\nAwesome! The file has been imported into the pipeline! Make sure you run the predefined functions block and then move to the next block to load your data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420d5f2",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "#### Predefined functions\n",
    "\n",
    "Here the code block below contains some predefined functions built for truncation, channel rejection, modified beer-lamberts law - signal processing, signal filtering and channel averaging. This code will run the functions so that it is available to use in the pipeline when you start working with the pipeline in future steps. It will not output anything rather than a success message that you are good to go to the next step.\n",
    "\n",
    "<font color=\"dodgerblue\"><b>You can however, go throught the code if you want to understand how the functions are made.</b> </font>\n",
    "\n",
    "##### <font color=\"tomato\">Make sure you run the code below before moving to the next block.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e2c0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awesome! Now run the next block for predefined analysis functions.\n"
     ]
    }
   ],
   "source": [
    "###### Pre-defined functions \n",
    "\n",
    "#Function for loading the menu of the pipeline\n",
    "def menu():\n",
    "    print(\"Menu - You can choose from the following options on which way would you like to go from here:\")\n",
    "    print(\"------------------------------------------------------------------------------\")\n",
    "    print(\"1. Data Truncation\")\n",
    "    print(\"2. Channel Rejection\")\n",
    "    print(\"3. MBLL (Signal Processing)\")\n",
    "    print(\"4. Signal Filtering\")\n",
    "    print(\"5. Channel Averaging\")\n",
    "    print(\"6. Analysis\")\n",
    "    print(\"7. Exit\")\n",
    "    return int(input())\n",
    "\n",
    "#Function for data-truncation\n",
    "def data_truncation(dataset):\n",
    "    print(\"Data Truncation:\")\n",
    "    while True:\n",
    "        truncate_method = int(input(\"How would you like to truncate your data: \\n 1. Cut off from the beginning\\n 2. Cut off from the end\\n 3. Crop from a certain time to a certain time. \\n\"))\n",
    "        \n",
    "        if truncate_method == 1:\n",
    "            # Code for Cutting off from the beginning\n",
    "            cutoff_start_time = float(input(\"Enter the time in seconds on how much time do you want to cut off from the start: \"))\n",
    "            dataset.crop(tmin=cutoff_start_time)\n",
    "            plt.rcParams[\"figure.figsize\"]= (6,6) #w,h\n",
    "            dataset.plot()\n",
    "            print(dataset.info)\n",
    "            return dataset\n",
    "            break    \n",
    "        elif truncate_method == 2:\n",
    "            # Code for cutting of from the end\n",
    "            cutoff_end_time = float(input(\"Enter the time in seconds on how much time do you want to cut off from the end: \"))\n",
    "            new_end_time = dataset.times[-1] - cutoff_end_time\n",
    "            dataset.crop(tmax=new_end_time)\n",
    "            plt.rcParams[\"figure.figsize\"]= (6,6) #w,h\n",
    "            dataset.plot()\n",
    "            print(dataset.info)\n",
    "            return dataset\n",
    "            break    \n",
    "        elif truncate_method == 3:\n",
    "            # Code for truncating the dataset between specific times\n",
    "            start_time = float(input(\"Enter starting time from where the dataset should start:\")) #defining start time \n",
    "            end_time = float(input(\"Enter the end time where the dataset should start:\")) #defining end time\n",
    "            dataset.crop(tmin=start_time, tmax=end_time)\n",
    "            plt.rcParams[\"figure.figsize\"]= (6,6) #w,h\n",
    "            dataset.plot()\n",
    "            print(dataset.info)\n",
    "            return dataset\n",
    "            break   \n",
    "        else:\n",
    "            print(\"Invalid choice. Please choose a valid option.\")\n",
    "\n",
    "#Function for CV Rejection \n",
    "def cv_channel_rejection(dataset):\n",
    "    print(\"Channel Rejection with 10%\")\n",
    "    while True:\n",
    "        ch_reject_method = int(input(\"How would you like to apply channel rejection: \\n 1. CV Threshold \\n\"))\n",
    "        \n",
    "        if ch_reject_method == 1:\n",
    "            # Code for channel rejection with cv threshold\n",
    "            num_samples = len(dataset.times)\n",
    "            threshold_value = num_samples * 0.10\n",
    "            \n",
    "            # Get a list of channels that do not meet the CV threshold\n",
    "            bad_channels = [ch_name for ch_name in dataset.ch_names if len(dataset.get_data(picks=ch_name)[0]) < threshold_value]\n",
    "            \n",
    "            #Identifying Bad Channels\n",
    "            print(\"Identifying bad channels\")\n",
    "            for channel in bad_channels:\n",
    "                print(channel)\n",
    "                \n",
    "            # Exclude the bad channels from the dataset\n",
    "            dataset = dataset.drop_channels(bad_channels)\n",
    "            \n",
    "            dataset.plot()\n",
    "            print(dataset.info)\n",
    "            return dataset\n",
    "            break    \n",
    "        else:\n",
    "            print(\"Invalid choice. Please choose a valid option.\")\n",
    "\n",
    "#Function for MBLL(HBO/HbR)\n",
    "def mbll(dataset):\n",
    "    print(\"Signal Processing: Converting from raw densities to optical densities first\\n\")\n",
    "    \n",
    "    #Converting the raw density to optical density\n",
    "    raw_od = mne.preprocessing.nirs.optical_density(dataset)\n",
    "    \n",
    "    #Plotting\n",
    "    plt.rcParams[\"figure.figsize\"]= (6,6) #w,h\n",
    "    print(\"Here is the visualization for the optical densities:\")\n",
    "    dataset.plot()\n",
    "    print(dataset.info)\n",
    "    \n",
    "    #Converting the optical densities to concentrations\n",
    "    from mne.preprocessing.nirs import beer_lambert_law\n",
    "    print(\"Now converting the optical densities into concentration with the help of Modified Beer-Lambert's Law\")\n",
    "    \n",
    "    #Using beer_lambert_law for converting into haemoglobin conentrations\n",
    "    raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od) \n",
    "    print(\"---------------------------------------------------------\")\n",
    "    \n",
    "    ##Plotting\n",
    "    print(\"Here is the visualization for the concentrations:\")\n",
    "    raw_haemo.plot()\n",
    "    print(raw_haemo.info)\n",
    "    print(f\"Channel Names:\\n {raw_haemo.ch_names} \\n\")\n",
    "    return raw_haemo\n",
    "\n",
    "#Function for signal filtration\n",
    "def signal_filtration(dataset):\n",
    "    while True:\n",
    "        sf_input = int(input(\"Choose what method do you want to go with:\\n 1.Motion Correction(TDDR)\\n 2.Normalization \\n\"))\n",
    "    \n",
    "        if sf_input == 1:\n",
    "            \n",
    "            #Code for motion correction using TDDR\n",
    "            tddr_input = int(input(\"What type of temporal filtering do you want to use?\\n 1. Low Pass (Gaussian Smoothing)\\n 2. High Pass (GLM Fourier)\"))\n",
    "\n",
    "            if tddr_input == 1:\n",
    "\n",
    "                print(\"\\n\\nApplying TDDR with low pass filtering(gaussian):\\n\")\n",
    "                \n",
    "                #Importing packages\n",
    "                from mne.preprocessing.nirs import optical_density, temporal_derivative_distribution_repair\n",
    "                import numpy as np\n",
    "\n",
    "                #Converting the raw density to optical density\n",
    "                raw_od = mne.preprocessing.nirs.optical_density(dataset)\n",
    "                \n",
    "                #Using beer_lambert_law for converting into haemoglobin conentrations\n",
    "                raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od) \n",
    "                \n",
    "                # Apply low-pass Gaussian smoothing with a cutoff frequency of 0.5 Hz\n",
    "                raw_haemo.filter(0.01, 0.5, method='iir')\n",
    "\n",
    "                # Apply tddr for automatic artifact detection and correction\n",
    "                corrected_tddr = temporal_derivative_distribution_repair(raw_haemo)\n",
    "                \n",
    "                # Visualize the results\n",
    "                raw_haemo.plot(n_channels=15, duration=400, show_scrollbars=False, title=\"Original Data\")\n",
    "                corrected_tddr.plot(n_channels=15, duration=400, show_scrollbars=False, title=\"Corrected Data\")\n",
    "                return(corrected_tddr)\n",
    "                break  \n",
    "                \n",
    "            elif tddr_input == 2:\n",
    "                \n",
    "                print(\"\\n\\nApplying TDDR with high pass filtering(GLM Fourier):\\n\")\n",
    "                \n",
    "                #Importing packages\n",
    "                from mne.preprocessing.nirs import optical_density, temporal_derivative_distribution_repair\n",
    "                import numpy as np\n",
    "\n",
    "                #Converting the raw density to optical density\n",
    "                raw_od = mne.preprocessing.nirs.optical_density(dataset)\n",
    "                \n",
    "                #Using beer_lambert_law for converting into haemoglobin conentrations\n",
    "                raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od)\n",
    "                \n",
    "                # Apply high-pass filtering with GLM Fourier filter\n",
    "                raw_haemo.filter(0.01, None, method='fir', fir_design='firwin', phase='zero-double')\n",
    "\n",
    "                # Apply tddr for automatic artifact detection and correction\n",
    "                corrected_tddr = temporal_derivative_distribution_repair(raw_haemo)\n",
    "\n",
    "                # Visualize the results\n",
    "                raw_haemo.plot(n_channels=15, duration=400, show_scrollbars=False, title=\"Original Data\")\n",
    "                corrected_tddr.plot(n_channels=15, duration=400, show_scrollbars=False, title=\"Corrected Data\")\n",
    "                \n",
    "                return(corrected_tddr)\n",
    "                break\n",
    "            else:\n",
    "                print(\"Wrong input. Please enter a valid number. \")\n",
    "                \n",
    "        elif sf_input == 2:\n",
    "            \n",
    "            #Code for normalization\n",
    "            print(\"\\n\\nApplying normalization with z-normalization and baseline-zero adjustment of 10:\\n\")\n",
    "            \n",
    "            #Importing packages\n",
    "            from mne.preprocessing.nirs import optical_density, temporal_derivative_distribution_repair\n",
    "            import numpy as np\n",
    "\n",
    "            #Converting the raw density to optical density\n",
    "            raw_od = mne.preprocessing.nirs.optical_density(dataset)\n",
    "            \n",
    "            # Step 2: Apply z-score normalization\n",
    "            mean_intensity = np.mean(raw_od.get_data(), axis=1, keepdims=True)\n",
    "            std_intensity = np.std(raw_od.get_data(), axis=1, keepdims=True)\n",
    "            z_scored_data = (raw_od.get_data() - mean_intensity) / std_intensity\n",
    "\n",
    "            # Step 3: Apply baseline-zero adjustment with a value of 10\n",
    "            baseline_value = 10\n",
    "            baseline_adjusted_data = z_scored_data - baseline_value\n",
    "            \n",
    "            # Create an MNE RawArray object for the baseline-adjusted data\n",
    "            raw_adjusted = mne.io.RawArray(baseline_adjusted_data, raw_od.info)\n",
    "\n",
    "            # Plot the baseline-adjusted data\n",
    "            raw_adjusted.plot(n_channels=15, duration=400, title=\"Baseline-Zero Adjusted Data\")\n",
    "            \n",
    "            return(raw_adjusted) \n",
    "            break\n",
    "        else:\n",
    "            print(\"Wrong input. Please enter a valid number. \")\n",
    "\n",
    "#Function for channel averaging\n",
    "def channel_averaging(dataset):\n",
    "    \n",
    "    print(\"\\nChannel Averaging is being applied:\\n\\n\")\n",
    "    \n",
    "    #Importing packages that are needed\n",
    "    from mne_nirs.preprocessing import channel_average\n",
    "    \n",
    "    #Converting the raw density to optical density\n",
    "    raw_od = mne.preprocessing.nirs.optical_density(dataset)\n",
    "    \n",
    "    #Converting the optical densities to concentrations\n",
    "    from mne.preprocessing.nirs import beer_lambert_law\n",
    "    \n",
    "    #Using beer_lambert_law for converting into haemoglobin conentrations\n",
    "    raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od) \n",
    "    \n",
    "    print(\"Stalled for now\")\n",
    "    \n",
    "print(\"Awesome! Now run the next block for predefined analysis functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c06cb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last step before the magic starts happening. Run the next block so that the pipeline takes in all the functions that is needed.\n"
     ]
    }
   ],
   "source": [
    "#Imports needed for design matrix\n",
    "import numpy as np\n",
    "from mne_nirs.experimental_design import make_first_level_design_matrix\n",
    "from mne_nirs.statistics import run_glm\n",
    "from mne_nirs.channels import (get_long_channels,\n",
    "                               get_short_channels,\n",
    "                               picks_pair_to_idx)\n",
    "\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "\n",
    "#Function for fnirs Analysis\n",
    "def analysis(dataset):\n",
    "    \n",
    "    while True:\n",
    "        analysis_input = int(input(\"Choose what type of analysis do you want to go with:\\n 1. GLM Analysis \\n 2. Functional Connectivity Analysis \\n 3. Effective Connectivity Analysis \\n 4. Waveform Average Analysis \\n \"))\n",
    "    \n",
    "        if analysis_input == 1:\n",
    "            \n",
    "            #Code for using GLM\n",
    "            \n",
    "            #Cleaning up annotations\n",
    "            raw_intensity.annotations.rename({'0': 'Rest',\n",
    "                                  '1': 'Walking'})\n",
    "            raw_intensity.annotations.set_durations(5)\n",
    "            \n",
    "            #Converting the raw density to optical density\n",
    "            raw_od = mne.preprocessing.nirs.optical_density(dataset)\n",
    "\n",
    "            #Plotting\n",
    "            plt.rcParams[\"figure.figsize\"]= (6,6) #w,h\n",
    "            print(\"Converting to optical densities:\")\n",
    "            #dataset.plot()\n",
    "            #print(dataset.info)\n",
    "\n",
    "            #Converting the optical densities to concentrations\n",
    "            from mne.preprocessing.nirs import beer_lambert_law\n",
    "            print(\"Now converting the optical densities into concentration with the help of Modified Beer-Lambert's Law\")\n",
    "\n",
    "            #Using beer_lambert_law for converting into haemoglobin conentrations\n",
    "            raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od) \n",
    "            print(\"---------------------------------------------------------\")\n",
    "\n",
    "            ##Plotting\n",
    "            print(\"Converting to the haemoglobin concentrations:\")\n",
    "            #raw_haemo.plot()\n",
    "            #print(raw_haemo.info)\n",
    "            #return raw_haemo\n",
    "            \n",
    "            #Selecting short channels and long channels\n",
    "            short_chs = get_short_channels(raw_haemo)\n",
    "            raw_haemo = get_long_channels(raw_haemo)\n",
    "            \n",
    "            #Viewing events\n",
    "            events, event_dict = mne.events_from_annotations(raw_haemo, verbose=False)\n",
    "            mne.viz.plot_events(events, event_id=event_dict, sfreq=raw_haemo.info['sfreq'])\n",
    "            \n",
    "            s = mne_nirs.experimental_design.create_boxcar(raw_haemo)\n",
    "            fig, ax = plt.subplots(figsize=(15, 6), constrained_layout=True)\n",
    "            ax.plot(raw_haemo.times, s)\n",
    "            ax.legend([\"Resting\", \"Walking\"], loc=\"upper right\")\n",
    "            ax.set_xlabel(\"Time (s)\");\n",
    "            \n",
    "            #Creating Design Matrix\n",
    "            print(\"\\nCreating design matrix\")\n",
    "            design_matrix = make_first_level_design_matrix(raw_haemo,\n",
    "                                               drift_model='cosine',\n",
    "                                               high_pass=0.005,  # Must be specified per experiment\n",
    "                                               hrf_model='spm',\n",
    "                                               stim_dur=5.0)\n",
    "            \n",
    "            design_matrix[\"ShortHbO\"] = np.mean(short_chs.copy().pick(\n",
    "                                    picks=\"hbo\").get_data(), axis=0)\n",
    "\n",
    "            design_matrix[\"ShortHbR\"] = np.mean(short_chs.copy().pick(\n",
    "                                    picks=\"hbr\").get_data(), axis=0)\n",
    "            \n",
    "            \n",
    "            fig, ax1 = plt.subplots(figsize=(10, 6), constrained_layout=True)\n",
    "            fig = plot_design_matrix(design_matrix, ax=ax1)\n",
    "            \n",
    "            #Examining expected response\n",
    "            fig, ax = plt.subplots(constrained_layout=True)\n",
    "            s = mne_nirs.experimental_design.create_boxcar(raw_intensity, stim_dur=5.0)\n",
    "            ax.plot(raw_intensity.times, s[:, 1])\n",
    "            ax.plot(design_matrix['Walking'])\n",
    "            ax.legend([\"Stimulus\", \"Expected Response\"])\n",
    "            ax.set(xlim=(0, 120), xlabel=\"Time (s)\", ylabel=\"Amplitude\")\n",
    "            \n",
    "            #Calculating glm estimation of the first two channels\n",
    "            channels_of_interest = ['S1_D1 hbo', 'S1_D1 hbr', 'S8_D6 hbo', 'S8_D6 hbr', 'S16_D13 hbo','S16_D13 hbr'] \n",
    "            data_subset = raw_haemo.copy().pick(picks=channels_of_interest)\n",
    "            glm_est = run_glm(data_subset, design_matrix)\n",
    "            print(glm_est)\n",
    "            \n",
    "            #Fitting glm to a copy of the single channel\n",
    "            glm_est.copy().pick('S1_D1 hbr')\n",
    "            \n",
    "            print(glm_est.copy().pick('S1_D1 hbr'))\n",
    "            \n",
    "            print(glm_est.MSE()) #Calculating mean square error of the two channels\n",
    "            print(glm_est.copy().pick('S1_D1 hbr').MSE()) #Calculating mean square error of a single channel\n",
    "            \n",
    "            #Displaying the first 9 rwos which corresponf to the 9 components of the design matrix for the first channel\n",
    "            print(glm_est.to_dataframe().head(9))\n",
    "            \n",
    "            glm_est.scatter()\n",
    "            \n",
    "            #Fitting glm to all data and viewing topographic distribution\n",
    "            glm_est = run_glm(raw_haemo, design_matrix)\n",
    "            glm_est.plot_topo(conditions=['Rest', 'Walking'])\n",
    "            \n",
    "            #To see exact activity spread on the head topowise based on hemispheres of hbo\n",
    "            fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 6), gridspec_kw=dict(width_ratios=[0.92, 1]))\n",
    "\n",
    "            glm_hbo = glm_est.copy().pick(picks=\"hbo\")\n",
    "            conditions = ['Walking']\n",
    "\n",
    "            glm_hbo.plot_topo(axes=axes[0], colorbar=True, conditions=conditions)\n",
    "\n",
    "            glm_hbo.copy().pick(picks=range(0,22)).plot_topo(conditions=conditions, axes=axes[1], colorbar=False)\n",
    "            glm_hbo.copy().pick(picks=range(23, 43)).plot_topo(conditions=conditions, axes=axes[1], colorbar=False)\n",
    "\n",
    "            axes[0].set_title(\"Smoothed across hemispheres - walking\")\n",
    "            axes[1].set_title(\"Hemispheres plotted independently - walking\")\n",
    "            \n",
    "            #Dorsal View \n",
    "            import os\n",
    "            os.environ[\"SUBJECTS_DIR\"] = \"C:/Users/Yameen/mne_data/MNE-sample-data\\subjects\" \n",
    "\n",
    "            glm_est.copy().surface_projection(condition=\"Walking\", view=\"dorsal\", chroma=\"hbo\")\n",
    "            \n",
    "            #Compute contrasts\n",
    "            contrast_matrix = np.eye(design_matrix.shape[1])\n",
    "            basic_conts = dict([(column, contrast_matrix[i])\n",
    "                   for i, column in enumerate(design_matrix.columns)])\n",
    "            contrast_LvR = basic_conts['Rest'] - basic_conts['Walking']\n",
    "\n",
    "            contrast = glm_est.compute_contrast(contrast_LvR)\n",
    "            contrast.plot_topo()\n",
    "            \n",
    "            #Exported into a dataframe \n",
    "            df = (glm_est.to_dataframe()\n",
    "                  .query('Condition in [\"Rest\", \"Walking\"]')\n",
    "                  .drop(['df', 'mse', 'p_value', 't'], axis=1)\n",
    "                  .groupby(['Condition', 'Chroma', 'ch_name'])\n",
    "                  .agg(['mean'])\n",
    "                 )\n",
    "\n",
    "            # Viewing the DataFrame\n",
    "            print(df)\n",
    "\n",
    "            # Exporting the dataFrame to a CSV file\n",
    "            #df.to_csv('E:/Yameen/Desktop/CSV files', index=False)\n",
    "            \n",
    "            return\n",
    "            \n",
    "        elif analysis_input == 2:\n",
    "            \n",
    "            #Code for using Functional Connectivty Analysis\n",
    "            \n",
    "            raw_intensity.annotations.rename({'0': 'Rest',\n",
    "                                  '1': 'Walking'})\n",
    "            raw_intensity.annotations.set_durations(5)\n",
    "            \n",
    "            #Converting the raw density to optical density\n",
    "            raw_od = mne.preprocessing.nirs.optical_density(dataset)\n",
    "\n",
    "            #Plotting\n",
    "            plt.rcParams[\"figure.figsize\"]= (6,6) #w,h\n",
    "            print(\"Converting to optical densities:\")\n",
    "            #dataset.plot()\n",
    "            #print(dataset.info)\n",
    "\n",
    "            #Converting the optical densities to concentrations\n",
    "            from mne.preprocessing.nirs import beer_lambert_law\n",
    "            print(\"Now converting the optical densities into concentration with the help of Modified Beer-Lambert's Law\")\n",
    "\n",
    "            #Using beer_lambert_law for converting into haemoglobin conentrations\n",
    "            raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od) \n",
    "            print(\"---------------------------------------------------------\")\n",
    "\n",
    "            ##Plotting\n",
    "            print(\"Converting to the haemoglobin concentrations:\")\n",
    "            #raw_haemo.plot()\n",
    "            #print(raw_haemo.info)\n",
    "            #return raw_haemo\n",
    "            \n",
    "            print(\"Functional Connectivity Analysis starts from here:\")\n",
    "            \n",
    "            from nilearn.connectome import ConnectivityMeasure\n",
    "            \n",
    "            # Initialize ConnectivityMeasure with kind='correlation'\n",
    "            connectivity_measure = ConnectivityMeasure(kind='correlation')\n",
    "\n",
    "            # Extract the time series data from your pre-processed dataset raw_haemo\n",
    "            time_series_data = np.array(raw_haemo.to_data_frame())\n",
    "\n",
    "            # Estimate the connectivity matrix\n",
    "            connectivity_matrix = connectivity_measure.fit_transform([time_series_data])[0]\n",
    "\n",
    "            # Plot the connectivity matrix\n",
    "            plt.imshow(connectivity_matrix, cmap='viridis', origin='lower')\n",
    "            plt.colorbar(label='Correlation')\n",
    "            plt.title('Functional Connectivity Matrix')\n",
    "            plt.xlabel('Channels')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.show()\n",
    "\n",
    "            #Task-Related Connectivity Modulation\n",
    "            \n",
    "            # Define task-related epochs\n",
    "            epochs_rest = raw_haemo.copy().crop(tmin=0, tmax=15)\n",
    "            epochs_task = raw_haemo.copy().crop(tmin=15, tmax=30)\n",
    "\n",
    "            # Extract time series data for rest and task epochs\n",
    "            time_series_rest = np.array(epochs_rest.to_data_frame())\n",
    "            time_series_task = np.array(epochs_task.to_data_frame())\n",
    "\n",
    "            # Compute connectivity matrices for rest and task epochs\n",
    "            connectivity_matrix_rest = connectivity_measure.fit_transform([time_series_rest])[0]\n",
    "            connectivity_matrix_task = connectivity_measure.fit_transform([time_series_task])[0]\n",
    "\n",
    "            # Compute difference matrix (task - rest)\n",
    "            connectivity_matrix_difference = connectivity_matrix_task - connectivity_matrix_rest\n",
    "\n",
    "            # Visualize connectivity matrices for rest, task, and their difference\n",
    "            plt.figure(figsize=(12, 4))\n",
    "\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(connectivity_matrix_rest, cmap='viridis', origin='lower')\n",
    "            plt.colorbar(label='Correlation')\n",
    "            plt.title('Functional Connectivity (Rest)')\n",
    "            plt.xlabel('Channels')\n",
    "            plt.ylabel('Channels')\n",
    "\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(connectivity_matrix_task, cmap='viridis', origin='lower')\n",
    "            plt.colorbar(label='Correlation')\n",
    "            plt.title('Functional Connectivity (Walking)')\n",
    "            plt.xlabel('Channels')\n",
    "            plt.ylabel('Channels')\n",
    "\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(connectivity_matrix_difference, cmap='coolwarm', origin='lower')\n",
    "            plt.colorbar(label='Difference')\n",
    "            plt.title('Walking - Rest Connectivity Difference')\n",
    "            plt.xlabel('Channels')\n",
    "            plt.ylabel('Channels')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            #Connectogram\n",
    "            \n",
    "            # Threshold\n",
    "            threshold = 0.96\n",
    "\n",
    "            # Creating a circular layout for the graph\n",
    "            theta = np.linspace(0, 2*np.pi, len(connectivity_matrix), endpoint=False)\n",
    "            x = np.cos(theta)\n",
    "            y = np.sin(theta)\n",
    "\n",
    "            # Plotting the connectogram\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.scatter(x, y, color='#1E90FF', s=500)  # Nodes with thinner edge\n",
    "            for i, (x_node, y_node) in enumerate(zip(x, y)):\n",
    "                plt.text(x_node, y_node, str(i), ha='center', va='center', fontsize=12, color='black', fontweight='bold')  # Channel numbers as labels\n",
    "            for i in range(len(connectivity_matrix)):\n",
    "                for j in range(i+1, len(connectivity_matrix)):\n",
    "                    if connectivity_matrix[i, j] >= threshold:  # Filter by threshold\n",
    "                        plt.plot([x[i], x[j]], [y[i], y[j]], color='#013c94', linewidth=2, alpha=0.8)  # Edges\n",
    "                        # Show channel numbers as labels\n",
    "                        plt.text((x[i] + x[j]) / 2, (y[i] + y[j]) / 2, f'{i}-{j}', ha='center', va='center', fontsize=12, color='black', fontweight='bold')\n",
    "            plt.title(f'Functional Connectivity Connectogram (Threshold: {threshold})', fontsize=16, fontweight='bold')\n",
    "            plt.axis('off')  # Turn off axis\n",
    "            plt.tight_layout()  # Adjust layout to make it look better\n",
    "            plt.show()\n",
    "\n",
    "            return\n",
    "        \n",
    "        elif analysis_input == 3:\n",
    "            \n",
    "            #Code for using Effective Connectivty Analysis\n",
    "            \n",
    "            raw_intensity.annotations.rename({'0': 'Rest',\n",
    "                                  '1': 'Walking'})\n",
    "            raw_intensity.annotations.set_durations(5)\n",
    "            \n",
    "            #Converting the raw density to optical density\n",
    "            raw_od = mne.preprocessing.nirs.optical_density(dataset)\n",
    "\n",
    "            #Plotting\n",
    "            plt.rcParams[\"figure.figsize\"]= (6,6) #w,h\n",
    "            print(\"Converting to optical densities:\")\n",
    "            #dataset.plot()\n",
    "            #print(dataset.info)\n",
    "\n",
    "            #Converting the optical densities to concentrations\n",
    "            from mne.preprocessing.nirs import beer_lambert_law\n",
    "            print(\"Now converting the optical densities into concentration with the help of Modified Beer-Lambert's Law\")\n",
    "\n",
    "            #Using beer_lambert_law for converting into haemoglobin conentrations\n",
    "            raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od) \n",
    "            print(\"---------------------------------------------------------\")\n",
    "\n",
    "            ##Plotting\n",
    "            print(\"Converting to the haemoglobin concentrations:\")\n",
    "            #raw_haemo.plot()\n",
    "            #print(raw_haemo.info)\n",
    "            #return raw_haemo\n",
    "            \n",
    "            print(\"Functional Connectivity Analysis starts from here:\")\n",
    "            \n",
    "            from nilearn.connectome import ConnectivityMeasure\n",
    "            \n",
    "            # Initialize ConnectivityMeasure with kind='correlation'\n",
    "            connectivity_measure = ConnectivityMeasure(kind='correlation')\n",
    "\n",
    "            # Extract the time series data from your pre-processed dataset raw_haemo\n",
    "            time_series_data = np.array(raw_haemo.to_data_frame())\n",
    "\n",
    "            # Estimate the connectivity matrix\n",
    "            connectivity_matrix = connectivity_measure.fit_transform([time_series_data])[0]\n",
    "\n",
    "            # Plot the connectivity matrix\n",
    "            plt.imshow(connectivity_matrix, cmap='viridis', origin='lower')\n",
    "            plt.colorbar(label='Correlation')\n",
    "            plt.title('Functional Connectivity Matrix')\n",
    "            plt.xlabel('Channels')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.show()\n",
    "            \n",
    "            ###### Effective Connectivity\n",
    "            print(\"Effective Connectivity Analysis starts from here:\")\n",
    "            import statsmodels.api as sm\n",
    "            \n",
    "            # Extract the haemoglobin concentration data\n",
    "            haemo_data = raw_haemo.get_data()\n",
    "            #print(haemo_data)\n",
    "            \n",
    "            # Assuming you have two channels of interest for effective connectivity analysis\n",
    "            # You can select these channels based on your study design and brain regions of interest\n",
    "            channel_1_data = haemo_data[0]  # Assuming channel 1\n",
    "            channel_5_data = haemo_data[5]  # Assuming channel 5\n",
    "\n",
    "            # Combine the data into a format suitable for Granger causality analysis\n",
    "            combined_data = np.vstack((channel_1_data, channel_5_data)).T\n",
    "\n",
    "            # Perform Granger causality analysis\n",
    "            max_lag = 5  # Adjust according to your data\n",
    "            granger_results = sm.tsa.stattools.grangercausalitytests(combined_data, max_lag, verbose=True)\n",
    "\n",
    "            #Visualize the time series data and Granger causality statistics as subplots\n",
    "            fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 8))\n",
    "\n",
    "            # Plot time series data\n",
    "            axes[0].plot(channel_1_data, label='Channel 1')\n",
    "            axes[0].plot(channel_5_data, label='Channel 5')\n",
    "            axes[0].set_xlabel('Time')\n",
    "            axes[0].set_ylabel('Haemoglobin Concentration')\n",
    "            axes[0].set_title('Time Series Data')\n",
    "            axes[0].legend()\n",
    "\n",
    "            # Plot Granger causality statistics\n",
    "            lags = range(1, max_lag + 1)\n",
    "            f_values = [granger_results[i + 1][0]['params_ftest'][0] for i in range(max_lag)]\n",
    "            axes[1].plot(lags, f_values, marker='o', linestyle='-')\n",
    "            axes[1].set_xlabel('Lag')\n",
    "            axes[1].set_ylabel('F-value')\n",
    "            axes[1].set_title('Granger Causality F-values')\n",
    "            axes[1].grid(True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Display the effective connectivity matrix in a separate plot\n",
    "            # Combine Measures\n",
    "            effective_connectivity_matrix = connectivity_matrix * granger_results[max_lag][0]['params_ftest'][0]\n",
    "\n",
    "            # Normalize the Combined Matrix (Optional)\n",
    "            effective_connectivity_matrix /= np.max(effective_connectivity_matrix)\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(effective_connectivity_matrix, cmap='viridis', origin='lower')\n",
    "            plt.colorbar(label='Effective Connectivity')\n",
    "            plt.title('Combined Functional Connectivity and Granger Causality Matrix')\n",
    "            plt.xlabel('Channels')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.show()\n",
    "            \n",
    "            # Frequency-domain analysis\n",
    "            from scipy.signal import coherence\n",
    "\n",
    "            # Define frequency bands (in Hz)\n",
    "            frequency_bands = {\n",
    "                'delta': (0.5, 4),\n",
    "                'theta': (4, 8),\n",
    "                'alpha': (8, 14),\n",
    "                'beta': (14, 30),\n",
    "                'gamma': (30, 40)\n",
    "            }\n",
    "\n",
    "            # Define the effective connectivity matrix\n",
    "            freq_effective_connectivity_matrix = effective_connectivity_matrix  # Assign your effective connectivity matrix here\n",
    "\n",
    "            # Compute coherence for each frequency band\n",
    "            coherence_results = {}\n",
    "            for band, (low, high) in frequency_bands.items():\n",
    "                # Extract the indices corresponding to the frequency band\n",
    "                low_idx = int(low / (raw_haemo.info['sfreq'] / len(freq_effective_connectivity_matrix)))\n",
    "                high_idx = int(high / (raw_haemo.info['sfreq'] / len(freq_effective_connectivity_matrix)))\n",
    "\n",
    "                # Extract the effective connectivity within the frequency band\n",
    "                freq_band_effective_connectivity = freq_effective_connectivity_matrix[low_idx:high_idx, low_idx:high_idx]\n",
    "\n",
    "                # Compute coherence as a measure of connectivity\n",
    "                coherence_results[band] = np.mean(freq_band_effective_connectivity)\n",
    "\n",
    "            # Visualize coherence for each frequency band\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.bar(coherence_results.keys(), coherence_results.values(), color='skyblue')\n",
    "            plt.xlabel('Frequency Band')\n",
    "            plt.ylabel('Coherence')\n",
    "            plt.title('Coherence in Different Frequency Bands')\n",
    "            plt.show()\n",
    "            \n",
    "            ##### Dynamic Connectivity starts from here\n",
    "            \n",
    "            #Segment Duration \n",
    "            segment_duration = 30\n",
    "            \n",
    "            # Get the total duration of the data\n",
    "            total_duration = raw_haemo.times[-1]\n",
    "\n",
    "            # Calculate the number of segments based on the specified duration\n",
    "            num_segments = int(np.ceil(total_duration / segment_duration))\n",
    "            \n",
    "            # Initialize lists to store connectivity matrices for each segment\n",
    "            connectivity_matrices = []\n",
    "\n",
    "            # Iterate over each segment\n",
    "            for i in range(num_segments):\n",
    "                # Determine the start and end times for the current segment\n",
    "                start_time = i * segment_duration\n",
    "                end_time = min((i + 1) * segment_duration, total_duration)\n",
    "\n",
    "                # Extract the data for the current segment\n",
    "                segment_data = raw_haemo.copy().crop(start_time, end_time)\n",
    "\n",
    "                # Functional Connectivity Analysis\n",
    "                connectivity_measure = ConnectivityMeasure(kind='correlation')\n",
    "                time_series_data = np.array(segment_data.to_data_frame())\n",
    "                connectivity_matrix = connectivity_measure.fit_transform([time_series_data])[0]\n",
    "\n",
    "                # Effective Connectivity Analysis\n",
    "                haemo_data = segment_data.get_data()\n",
    "                channel_1_data = haemo_data[0]  # Assuming channel 1\n",
    "                channel_5_data = haemo_data[5]  # Assuming channel 5\n",
    "                combined_data = np.vstack((channel_1_data, channel_5_data)).T\n",
    "                max_lag = 5  # Adjust according to your data\n",
    "                granger_results = sm.tsa.stattools.grangercausalitytests(combined_data, max_lag, verbose=False)\n",
    "\n",
    "                # Combine Measures and append the connectivity matrix to the list\n",
    "                effective_connectivity_matrix = connectivity_matrix * granger_results[max_lag][0]['params_ftest'][0]\n",
    "                effective_connectivity_matrix /= np.max(effective_connectivity_matrix)\n",
    "                connectivity_matrices.append(effective_connectivity_matrix)\n",
    "            \n",
    "            \n",
    "            # Visualize the dynamic connectivity matrices\n",
    "            plt.figure(figsize=(25, 15))\n",
    "            for i, matrix in enumerate(connectivity_matrices):\n",
    "                plt.subplot(1, num_segments, i + 1)\n",
    "                plt.imshow(matrix, cmap='viridis', origin='lower')\n",
    "                plt.colorbar(label='Effective Connectivity')\n",
    "                plt.title(f'Segment {i + 1}')\n",
    "                plt.xlabel('Channels')\n",
    "                plt.ylabel('Channels')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            #### Graph theories\n",
    "            \n",
    "            # Compute the mean effective dynamic connectivity matrix over time\n",
    "            mean_connectivity_matrix = np.mean(connectivity_matrices, axis=0)\n",
    "\n",
    "            # Visualize the mean connectivity matrix\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(mean_connectivity_matrix, cmap='viridis', origin='lower')\n",
    "            plt.colorbar(label='Mean Effective Dynamic Connectivity')\n",
    "            plt.title('Mean Effective Connectivity Matrix')\n",
    "            plt.xlabel('Channels')\n",
    "            plt.ylabel('Channels')\n",
    "            plt.show()\n",
    "            \n",
    "            return\n",
    "            \n",
    "        elif analysis_input == 4:\n",
    "            \n",
    "            #Code for using Waveform Average Analysis\n",
    "            \n",
    "            # Rename annotations\n",
    "            raw_intensity.annotations.rename({'0': 'Rest', '1': 'Walking'})\n",
    "\n",
    "            #Selecting channels \n",
    "            picks = mne.pick_types(raw_intensity.info, meg=False, fnirs=True)\n",
    "            dists = mne.preprocessing.nirs.source_detector_distances(\n",
    "                raw_intensity.info, picks=picks)\n",
    "            raw_intensity.pick(picks[dists > 0.01])\n",
    "            raw_intensity.plot(n_channels=len(raw_intensity.ch_names),\n",
    "                               duration=500, show_scrollbars=False)\n",
    "            \n",
    "            \n",
    "            # Convert the raw density to optical density\n",
    "            raw_od = mne.preprocessing.nirs.optical_density(dataset)\n",
    "            \n",
    "            #SCI\n",
    "            sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.hist(sci)\n",
    "            ax.set(xlabel='Scalp Coupling Index', ylabel='Count', xlim=[0, 1])\n",
    "\n",
    "            #Compress \n",
    "            from itertools import compress\n",
    "            raw_od.info['bads'] = list(compress(raw_od.ch_names, sci < 0.5))\n",
    "            \n",
    "            # Convert the optical densities to concentrations\n",
    "            from mne.preprocessing.nirs import beer_lambert_law\n",
    "\n",
    "            raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od, ppf=0.1) \n",
    "            raw_haemo.plot(n_channels=len(raw_haemo.ch_names), duration=500, show_scrollbars=False)\n",
    "\n",
    "            # HeartRateFilter\n",
    "            fig = raw_haemo.plot_psd(average=True)\n",
    "            fig.suptitle('Before filtering', weight='bold', size='x-large')\n",
    "            raw_haemo = raw_haemo.filter(0.05, 0.7, h_trans_bandwidth=0.2,\n",
    "                                         l_trans_bandwidth=0.02)\n",
    "            fig = raw_haemo.plot_psd(average=True)\n",
    "            fig.suptitle('After filtering', weight='bold', size='x-large')\n",
    "            \n",
    "            #Extracting epochs\n",
    "            events, event_dict = mne.events_from_annotations(raw_haemo)\n",
    "            fig = mne.viz.plot_events(events, event_id=event_dict, sfreq=raw_haemo.info['sfreq'])\n",
    "            \n",
    "            #Epochs\n",
    "            reject_criteria = dict(hbo=100e-6, hbr=100e-6)\n",
    "            tmin, tmax = -5, 15\n",
    "\n",
    "            epochs = mne.Epochs(raw_haemo, events, event_id=event_dict,\n",
    "                                tmin=tmin, tmax=tmax,\n",
    "                                reject=reject_criteria, reject_by_annotation=True,\n",
    "                                proj=True, baseline=(None, 0), preload=True,\n",
    "                                detrend=None, verbose=True)\n",
    "            #print(epochs.drop_log)\n",
    "            epochs.plot_drop_log()\n",
    "            \n",
    "            epochs['Walking'].plot_image(combine='mean', vmin=-30, vmax=30,\n",
    "                             ts_args=dict(ylim=dict(hbo=[-15, 15],\n",
    "                                                    hbr=[-15, 15])))\n",
    "            \n",
    "            epochs['Rest'].plot_image(combine='mean', vmin=-30, vmax=30,\n",
    "                             ts_args=dict(ylim=dict(hbo=[-15, 15],\n",
    "                                                    hbr=[-15, 15])))\n",
    "            \n",
    "            #Viewing consistency of responses across chanels\n",
    "            fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 6))\n",
    "            clims = dict(hbo=[-20, 20], hbr=[-20, 20])\n",
    "            epochs['Rest'].average().plot_image(axes=axes[:, 0], clim=clims)\n",
    "            epochs['Walking'].average().plot_image(axes=axes[:, 1], clim=clims)\n",
    "            for column, condition in enumerate(['Rest', 'Walking']):\n",
    "                for ax in axes[:, column]:\n",
    "                    ax.set_title('{}: {}'.format(condition, ax.get_title()))\n",
    "                    \n",
    "            #Plotting Standard FNIRS Response Image\n",
    "            evoked_dict = {'Walking/HbO': epochs['Walking'].average(picks='hbo'),\n",
    "                           'Walking/HbR': epochs['Walking'].average(picks='hbr'),\n",
    "                           'Rest/HbO': epochs['Rest'].average(picks='hbo'),\n",
    "                           'Rest/HbR': epochs['Rest'].average(picks='hbr')}\n",
    "\n",
    "            # Rename channels until the encoding of frequency in ch_name is fixed\n",
    "            for condition in evoked_dict:\n",
    "                evoked_dict[condition].rename_channels(lambda x: x[:-4])\n",
    "\n",
    "            color_dict = dict(HbO='#AA3377', HbR='b')\n",
    "            styles_dict = dict(Rest=dict(linestyle='dashed'))\n",
    "\n",
    "            mne.viz.plot_compare_evokeds(evoked_dict, combine=\"mean\", ci=0.95,\n",
    "                                         colors=color_dict, styles=styles_dict)\n",
    "            \n",
    "            #Topographic View for walking\n",
    "            times = np.arange(-3.5, 13.2, 3.0)\n",
    "            topomap_args = dict(extrapolate='local')\n",
    "            epochs['Walking'].average(picks='hbo').plot_joint(\n",
    "                times=times, topomap_args=topomap_args)\n",
    "            \n",
    "            #Comparison by topographic view\n",
    "            times = np.arange(4.0, 11.0, 1.0)\n",
    "            epochs['Rest'].average(picks='hbo').plot_topomap(\n",
    "                times=times, **topomap_args)\n",
    "            epochs['Walking'].average(picks='hbo').plot_topomap(\n",
    "                times=times, **topomap_args)\n",
    "            \n",
    "            #Individual Waverforms in togographics view \n",
    "            fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
    "            mne.viz.plot_evoked_topo(epochs['Rest'].average(picks='hbo'), color='b',\n",
    "                                     axes=axes, legend=False)\n",
    "            mne.viz.plot_evoked_topo(epochs['Walking'].average(picks='hbo'), color='r',\n",
    "                                     axes=axes, legend=False)\n",
    "\n",
    "            # Tidy the legend:\n",
    "            leg_lines = [line for line in axes.lines if line.get_c() == 'b'][:1]\n",
    "            leg_lines.append([line for line in axes.lines if line.get_c() == 'r'][0])\n",
    "            fig.legend(leg_lines, ['Rest', 'Walking'], loc='lower right')\n",
    "\n",
    "            return \n",
    "        else:\n",
    "            print(\"Wrong input. Please enter a valid number. \")\n",
    "\n",
    "print(\"Last step before the magic starts happening. Run the next block so that the pipeline takes in all the functions that is needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b8c9974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! You can proceed to next block for loading your dataset!\n"
     ]
    }
   ],
   "source": [
    "data_operations = {\n",
    "    1: data_truncation,\n",
    "    2: cv_channel_rejection,\n",
    "    3: mbll,\n",
    "    4: signal_filtration,\n",
    "    5: channel_averaging,\n",
    "    6: analysis\n",
    "}\n",
    "\n",
    "data = []\n",
    "\n",
    "print(\"Great! You can proceed to next block for loading your dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a582e",
   "metadata": {},
   "source": [
    " -------------------------------\n",
    " #### For Loading SNIRF Data\n",
    " \n",
    "The code below will read and load your snirf file in terms of informations about the dataset and also in visualization as a plot. The visualization as mentioned before will come on a separate window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06dfcf36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:\\Users\\Yameen\\mne_data\\infinity-walking-data\\2021-10-16_001\\2021-10-16_001.snirf\n",
      "Reading 0 ... 1657  =      0.000 ...   162.883 secs...\n",
      "\n",
      "Your dataset has been loaded! Here is the visualization for it:\n",
      "Using matplotlib as 2D backend.\n",
      "\n",
      "Do you want to know more about the dataset (Yes/No)?\n",
      "No\n",
      "You can continue to the next block and run it to decide which options do you want to go further with in the pipeline\n"
     ]
    }
   ],
   "source": [
    "#Loading/Reading the snirf data - preload as we are going to load the data and verbose for more details\n",
    "raw_intensity = mne.io.read_raw_snirf(fname,optode_frame='mri',preload=True, verbose=True)\n",
    "data.append(raw_intensity)\n",
    "\n",
    "print(\"\\nYour dataset has been loaded! Here is the visualization for it:\")\n",
    "plt.rcParams[\"figure.figsize\"]= (6,6) #w,h\n",
    "# raw_intensity.plot_sensors()\n",
    "raw_intensity.plot()\n",
    "\n",
    "knowmore = \"\"\n",
    "while knowmore.lower() not in [\"yes\", \"no\"]:\n",
    "    knowmore = input(\"\\nDo you want to know more about the dataset (Yes/No)?\\n\")\n",
    "\n",
    "if knowmore.lower() == \"yes\":\n",
    "    print(f\"Information about the dataset:\\n {raw_intensity.info} \\n\")\n",
    "    print(f\"Channel Names:\\n {raw_intensity.ch_names} \\n\")\n",
    "    print(f\"Annotations:\\n {raw_intensity.annotations} \\n\")\n",
    "    print(\"Here is the dataset:\")\n",
    "    data = raw_intensity.get_data()\n",
    "    print(data)\n",
    "    print(\"------------------------------------------------------------------------\\n\")\n",
    "    print(\"You can continue to the next block and run it to see which options you want to go further with in the pipeline\")\n",
    "elif knowmore.lower() == \"no\": \n",
    "    print(\"You can continue to the next block and run it to decide which options do you want to go further with in the pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82a424e",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "#### Proceeding with the pipeline\n",
    "\n",
    "This is where the magic happens! This is where your journey in the pipeline starts. Your dataset is loaded. You can proceed with the pipeline, choose what you want to do with it. Happy <font color=\"tomato\">**fNIRS-ing**</font>!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf9985",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menu - You can choose from the following options on which way would you like to go from here:\n",
      "------------------------------------------------------------------------------\n",
      "1. Data Truncation\n",
      "2. Channel Rejection\n",
      "3. MBLL (Signal Processing)\n",
      "4. Signal Filtering\n",
      "5. Channel Averaging\n",
      "6. Analysis\n",
      "7. Exit\n",
      "6\n",
      "Choose what type of analysis do you want to go with:\n",
      " 1. GLM Analysis \n",
      " 2. Functional Connectivity Analysis \n",
      " 3. Effective Connectivity Analysis \n",
      " 4. Waveform Average Analysis \n",
      " 1\n",
      "Converting to optical densities:\n",
      "Now converting the optical densities into concentration with the help of Modified Beer-Lambert's Law\n",
      "---------------------------------------------------------\n",
      "Converting to the haemoglobin concentrations:\n",
      "Used Annotations descriptions: ['Rest', 'Walking']\n",
      "\n",
      "Creating design matrix\n",
      "Used Annotations descriptions: ['Rest', 'Walking']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yameen\\anaconda3\\lib\\site-packages\\nilearn\\plotting\\matrix_plotting.py:425: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLM Results for 6 channels\n",
      "GLM Results for 1 channels\n",
      "[2.3379204459238827e-14, 2.523639399174592e-15, 5.621390606245392e-15, 6.429900678891942e-16, 1.4541048429133836e-14, 9.145879931311042e-15]\n",
      "[2.523639399174592e-15]\n",
      "variable Condition   df           mse   p_value            se          t  \\\n",
      "12            Rest  6.0  2.337920e-14  0.011068  7.773931e-08   3.622220   \n",
      "13        ShortHbO  6.0  2.337920e-14  0.704678  2.946323e-03  -0.397610   \n",
      "14        ShortHbR  6.0  2.337920e-14  0.328338  2.173180e-03  -1.063823   \n",
      "15         Walking  6.0  2.337920e-14  0.000441  8.757017e-08   6.947220   \n",
      "16        constant  6.0  2.337920e-14  0.013284  3.027878e-08  -3.471237   \n",
      "17         drift_1  6.0  2.337920e-14  0.000009  9.069433e-07  13.779281   \n",
      "18            Rest  6.0  2.523639e-15  0.007752  2.554109e-08  -3.925552   \n",
      "19        ShortHbO  6.0  2.523639e-15  0.060053  9.680083e-04  -2.312616   \n",
      "20        ShortHbR  6.0  2.523639e-15  0.031015  7.139938e-04  -2.803639   \n",
      "\n",
      "variable         theta  Source  Detector Chroma  Significant    ch_name  \n",
      "12        2.815889e-07       1         1    hbo         True  S1_D1 hbo  \n",
      "13       -1.171486e-03       1         1    hbo        False  S1_D1 hbo  \n",
      "14       -2.311878e-03       1         1    hbo        False  S1_D1 hbo  \n",
      "15        6.083692e-07       1         1    hbo         True  S1_D1 hbo  \n",
      "16       -1.051048e-07       1         1    hbo         True  S1_D1 hbo  \n",
      "17        1.249703e-05       1         1    hbo         True  S1_D1 hbo  \n",
      "18       -1.002629e-07       1         1    hbr         True  S1_D1 hbr  \n",
      "19       -2.238632e-03       1         1    hbr        False  S1_D1 hbr  \n",
      "20       -2.001780e-03       1         1    hbr         True  S1_D1 hbr  \n",
      "variable                                se         theta Source Detector  \\\n",
      "                                      mean          mean   mean     mean   \n",
      "Condition Chroma ch_name                                                   \n",
      "Rest      hbo    S10_D10 hbo  8.698700e-08  8.539380e-07   10.0     10.0   \n",
      "                 S10_D8 hbo   9.213093e-08  8.660691e-07   10.0      8.0   \n",
      "                 S11_D11 hbo  2.931076e-07 -8.830739e-07   11.0     11.0   \n",
      "                 S11_D9 hbo   1.214856e-07  1.040975e-07   11.0      9.0   \n",
      "                 S12_D10 hbo  8.730053e-08  5.015965e-07   12.0     10.0   \n",
      "...                                    ...           ...    ...      ...   \n",
      "Walking   hbr    S7_D7 hbr    7.525350e-08 -1.173528e-07    7.0      7.0   \n",
      "                 S8_D6 hbr    2.758707e-08 -2.764950e-08    8.0      6.0   \n",
      "                 S8_D7 hbr    5.445909e-08 -4.863469e-08    8.0      7.0   \n",
      "                 S9_D8 hbr    5.630621e-08 -1.873782e-07    9.0      8.0   \n",
      "                 S9_D9 hbr    4.679706e-08 -9.059473e-08    9.0      9.0   \n",
      "\n",
      "variable                     Significant  \n",
      "                                    mean  \n",
      "Condition Chroma ch_name                  \n",
      "Rest      hbo    S10_D10 hbo         1.0  \n",
      "                 S10_D8 hbo          1.0  \n",
      "                 S11_D11 hbo         1.0  \n",
      "                 S11_D9 hbo          0.0  \n",
      "                 S12_D10 hbo         1.0  \n",
      "...                                  ...  \n",
      "Walking   hbr    S7_D7 hbr           0.0  \n",
      "                 S8_D6 hbr           0.0  \n",
      "                 S8_D7 hbr           0.0  \n",
      "                 S9_D8 hbr           1.0  \n",
      "                 S9_D9 hbr           0.0  \n",
      "\n",
      "[176 rows x 5 columns]\n",
      "Operation completed successfully!\n",
      "\n",
      "\n",
      "Menu - You can choose from the following options on which way would you like to go from here:\n",
      "------------------------------------------------------------------------------\n",
      "1. Data Truncation\n",
      "2. Channel Rejection\n",
      "3. MBLL (Signal Processing)\n",
      "4. Signal Filtering\n",
      "5. Channel Averaging\n",
      "6. Analysis\n",
      "7. Exit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    option = menu()\n",
    "    if option == 7:\n",
    "        break\n",
    "    elif option in data_operations:\n",
    "        processed_data = data_operations[option](data[0])  # Passing original data\n",
    "        data.append(processed_data)\n",
    "        print(\"Operation completed successfully!\" + \"\\n\\n\")\n",
    "    else:\n",
    "        print(\"Invalid option! Choose an option from the menu (1-6): \")\n",
    "\n",
    "print(\"Thank you for using the pipeline!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
